# Codex CLI Configuration File
# Place in container at /root/.codex/config.toml
# Users should customize this file for their own setup

# Default model provider
model_provider = "openai"

# Local Ollama provider configuration
[model_providers.ollama]
name = "Ollama"
base_url = "http://localhost:11434/v1"  # Change to your Ollama host

# LM Studio provider configuration (alternative)
[model_providers.lmstudio]
name = "LM Studio"
base_url = "http://localhost:1234/v1"

# Profile: Use Ollama with qwen3:14b
[profiles.ollama-qwen]
model_provider = "ollama"
model = "qwen3:14b"

# Profile: Use OpenAI
[profiles.openai]
model_provider = "openai"
model = "gpt-4o"

# Sandbox configuration
[sandbox]
# Options: read-only, workspace-write, danger-full-access
mode = "workspace-write"

# Approval policy
[approval]
# Options: untrusted, on-failure, on-request, never
mode = "never"
